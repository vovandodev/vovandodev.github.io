<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Clouds, Virtualization and something more..."><meta name=Author content="Volodymyr Vrublevskyy"><meta name=keywords content="IT Talks"><link rel=stylesheet href=https://vovando.dev/css/syntax.css><link rel=stylesheet href=https://vovando.dev/css/style.css><script src=https://kit.fontawesome.com/1b7478c139.js crossorigin=anonymous></script><title>IT Talks</title></head><body><aside id=sidenav><header><a href=https://vovando.dev/><img src=https://vovando.dev/avatar.png alt=avatar></a>
<a id=branding href=https://vovando.dev/>IT Talks</a></header><nav><a href=/><i class="fas fa-home fa-sm"></i><span>whoami</span></a>
<a href=/posts><i class="fas fa-keyboard fa-ms"></i><span>blog</span></a>
<a href=/tags><i class="fas fa-tags fa-ms"></i><span>tags</span></a>
<a href=https://github.com/vovandodev target=_blank><i class="fab fa-github fa-ms"></i><span>github</span></a>
<a href=/index.xml><i class="fas fa-rss fa-ms"></i><span>RSS</span></a>
<a href=/contact><i class="far fa-envelope"></i><span>contact</span></a></nav></aside><main id=main><a href=javascript:void(0) id=closebtn onclick=navToggle()><i class="fas fa-bars fa-lg"></i></a><div class=content><h1 id=title>Embedded VSAN Cluster</h1><nav id=TableOfContents></nav><p>In the nearest future I am planing to update vSAN to 6.0 version on Production. The procedure is not trivial, so I want to try update on test lab. We need to build an entire embedded vsan environment.</p><p><strong>Some facts about vSAN 5.5.</strong></p><blockquote><p>Virtual SAN 5.5 requirements:</p><p>– a minimum of 3 hosts contributing local disks, at least one SSD and one HDD per host (not all hosts need to contribute)</p><p>– ESXi 5.5 or later on each host</p><p>– hosts must be managed by vCenter Server 5.5</p><p>– 6GB of RAM on each host (10GB for 7 or more disks per host)</p><p>– compatbile storage devices listed on the HCL</p><p>– SSDs must be in use by Flash Read Cache or formatted with any filesystem</p><p>– a 1:10 ratio of SSD capacity to HDD in each host</p><p>– an SAS or SATA HBA, or RAID controller that is set up in RAID0 mode or passthrough</p><p>– ESXi Dump Collector and Syslog Collector must be configured to redirect memory dumps and logs onto a network server</p><p>– all hosts in a VSAN cluster must be part of a VSAN network and must be on the same subnet</p><p>– a private 1GbE IPv4 network, preferably 10GbE with at least one physical NIC dedicated to VSAN on each host</p><p>– VMkernel port groups must have the VSAN port property activated</p><p>DRS and HA supported.</p><p>Fault Tolerance, storage DRS, deduplication and Metro Clustering are NOT supported.</p><p>Virtual SAN 5.5 maximums:</p><p>– 32 hosts in a cluster</p><p>– 1 VSAN datastore per cluster</p><p>– 5 disk groups per host</p><p>– 35 HDDs per host, 7 HDDs per disk group</p><p>– 1 SSD per disk group</p><p>– 3000 components per host</p><p>– 3200 VMs per cluster, 100 VMs per host</p><p>– 2TB VMDK size</p><p>– 2 dedicated physical networks</p><p>– 3 host failures the cluster can tolerate</p><p>NOTE: on a cluster with HA, the maximum number of VMs per host is 64 and per cluster 2048.</p></blockquote><p> </p><p><strong>So let’s start to build my test environment.</strong></p><ol><li>First, I took one of my physical esxi servers. This server  simply had 12 cpus, a few network cards (only need one here) and 96GB of memory.</li></ol><p>Typical install, then I used the dcui and changed it&rsquo;s ip to 192.168.95.120/24 and called it esxi.vovando.com.</p><p>I used:</p><p>Product: VMware ESXi 5.5</p><p>Release date: February 5, 2015</p><p>Patch: ESXi550-201502001</p><p>Build: 2456374</p><p> </p><p> </p><p></p><p> </p><p>It is <strong>CRITICAL</strong> to remember to change <strong>Promiscous Mode to ACCEPT</strong> to allow for internal communication. These switches had VM networks: vMotion-test and vSan-test.</p><p> </p><p></p><p> </p><ol start=3><li>Now, it’s time to create the three embedded esxi servers.  Using the vsphere client, I created three identical esxi servers (esxi1 – 192.168.95.131), (esxi2 – 192.168.95.132) and (esxi3 – 192.168.95.133). I gave each embedded esxi server 2 vcpus, 5 gbs of ram and 3 disks. The 3 disks were 4gbs for the boot disk, 10gbs for the ssd disk   and 100gbs for the HDD to be used by vsan.</li></ol><p></p><p> </p><ol start=4><li>After the installation of each of the embedded esxi servers, I changed the disk type of each of the future ssd disks using the esxcli command. Here are the commands:</li></ol><p> </p><p><em>$ esxcli storage nmp satp rule add –satp=VMW_SATP_LOCAL –device mpx.vmhba1:C0:T1:L0 –option &ldquo;enable_local enable_ssd&rdquo;</em></p><p><em>$ esxcli storage core claiming unclaim –type=device –device mpx.vmhba1:C0:T1:L0</em></p><p>You need to reboot host after it.</p><p> </p><ol start=5><li>Now, it was time to create the vcenter server. I used the windows-based vcenter server. Ip address that was 192.168.95.130.</li></ol><p> </p><ol start=7><li><p>Connected to the vcenter server with the vsphere client, logged in and I created a datacenter and added the three esxi servers.</p></li><li><p>Using the vsphere client, it was time to create the vmotion network and the vsan network, which can&rsquo;t be created with the vsphere client only.</p></li></ol><p><strong>vmotion</strong></p><p>esxi01 – 10.1.0.131</p><p>esxi02 – 10.1.0.132</p><p>esxi03 – 10.1.0.133</p><p><strong>vsan</strong></p><p>esxi01 – 10.2.0.131</p><p>esxi02 – 10.2.0.132</p><p>esxi03 – 10.2.0.133</p><p></p><p>9.  Using the web client (port 9443), create a cluster for vsan and create the disk groups necessary. If you select manual configuration, you need to create three disk groups (one per host) using the fake ssd disk and the fake HDD disk. By the time you are done, you should have a datastore that is roughly 300gb. While creating the cluster, I also enabled DRS (fully automatic) and HA (which uses the vsan network for heartbeats).</p><p></p><p>After that this <strong>Cluster</strong> can be used for work.</p><p> </p><ol start=10><li>vSan related commands.</li></ol><p>#<strong>esxcli vsan</strong></p><p>Usage: esxcli vsan {cmd} [cmd options]</p><p>Available Namespaces:</p><p>  datastore             Commands for VSAN datastore configuration</p><p>  network               Commands for VSAN host network configuration</p><p>  storage               Commands for VSAN physical storage configuration</p><p>  cluster               Commands for VSAN host cluster configuration</p><p>  faultdomain           Commands for VSAN fault domain configuration</p><p>  maintenancemode       Commands for VSAN maintenance mode operation</p><p>  policy                Commands for VSAN storage policy configuration</p><p>  trace                 Commands for VSAN trace configuration</p><p> </p><p>#<strong>esxcli vsan datastore name get</strong></p><p>   Name: vsanDatastore</p><p>#<strong>esxcli vsan network ipv4 add -i vmk2</strong></p><p>This VMKernel network interface is already in use. Can&rsquo;t add again. If you want to reconfigure, use &lsquo;set&rsquo; command.</p><p>#<strong>esxcli vsan network list</strong></p><p>Interface</p><p>   VmkNic Name: vmk2</p><p>   IP Protocol: IPv4</p><p>   Interface UUID: 26e84855-f8e9-f3ad-5ff8-005056873a81</p><p>   Agent Group Multicast Address: 224.2.3.4</p><p>   Agent Group Multicast Port: 23451</p><p>   Master Group Multicast Address: 224.1.2.3</p><p>   Master Group Multicast Port: 12345</p><p>   Multicast TTL: 5</p><p> </p><p># <strong>esxcli vsan network list</strong></p><p>Interface</p><p>   VmkNic Name: vmk2</p><p>   IP Protocol: IPv4</p><p>   Interface UUID: 26e84855-f8e9-f3ad-5ff8-005056873a81</p><p>   Agent Group Multicast Address: 224.2.3.4</p><p>   Agent Group Multicast Port: 23451</p><p>   Master Group Multicast Address: 224.1.2.3</p><p>   Master Group Multicast Port: 12345</p><p>   Multicast TTL: 5</p><p># <strong>esxcli vsan cluster get</strong></p><p>Cluster Information</p><p>   Enabled: true</p><p>   Current Local Time: 2015-05-07T16:14:52Z</p><p>   Local Node UUID: 5548e218-ae75-c834-0331-005056873a81</p><p>   Local Node State: BACKUP</p><p>   Local Node Health State: HEALTHY</p><p>   Sub-Cluster Master UUID: 5548e429-d572-3814-2780-005056872eec</p><p>   Sub-Cluster Backup UUID: 5548e218-ae75-c834-0331-005056873a81</p><p>   Sub-Cluster UUID: 523a032b-4d44-8fbd-2954-74154216d6b2</p><p>   Sub-Cluster Membership Entry Revision: 11</p><p>   Sub-Cluster Member UUIDs: 5548e429-d572-3814-2780-005056872eec, 5548e218-ae75-c834-0331-005056873a81, 5548de32-1dc9-573c-6be5-005056877536</p><p>   Sub-Cluster Membership UUID: f68c4b55-ec10-02a5-120c-005056872eec</p><p># <strong>esxcli vsan storage list</strong></p><p>mpx.vmhba1:C0:T2:L0</p><p>   Device: mpx.vmhba1:C0:T2:L0</p><p>   Display Name: mpx.vmhba1:C0:T2:L0</p><p>   Is SSD: false</p><p>   VSAN UUID: 52711c36-d9a2-c17b-7b4f-fe6ca685d0e8</p><p>   VSAN Disk Group UUID: 52887c5a-12a2-384d-679b-fb9ed6775bfa</p><p>   VSAN Disk Group Name: mpx.vmhba1:C0:T1:L0</p><p>   Used by this host: true</p><p>   In CMMDS: true</p><p>   Checksum: 5186061164308225788</p><p>   Checksum OK: true</p><p>   Emulated DIX/DIF Enabled: false</p><p>mpx.vmhba1:C0:T1:L0</p><p>   Device: mpx.vmhba1:C0:T1:L0</p><p>   Display Name: mpx.vmhba1:C0:T1:L0</p><p>   Is SSD: true</p><p>   VSAN UUID: 52887c5a-12a2-384d-679b-fb9ed6775bfa</p><p>   VSAN Disk Group UUID: 52887c5a-12a2-384d-679b-fb9ed6775bfa</p><p>   VSAN Disk Group Name: mpx.vmhba1:C0:T1:L0</p><p>   Used by this host: true</p><p>   In CMMDS: true</p><p>   Checksum: 10790749511316735198</p><p>   Checksum OK: true</p><p>   Emulated DIX/DIF Enabled: false</p><p>#<strong>esxcli vsan policy getdefault</strong></p><p>Policy Class  Policy Value</p><p>————  ——————————————————–</p><p>cluster       ((&ldquo;hostFailuresToTolerate&rdquo; i1))</p><p>vdisk         ((&ldquo;hostFailuresToTolerate&rdquo; i1))</p><p>vmnamespace   ((&ldquo;hostFailuresToTolerate&rdquo; i1))</p><p>vmswap        ((&ldquo;hostFailuresToTolerate&rdquo; i1) (&ldquo;forceProvisioning&rdquo; i1))</p><p>vmem          ((&ldquo;hostFailuresToTolerate&rdquo; i1) (&ldquo;forceProvisioning&rdquo; i1))</p><p>#grep -i vsan /etc/vmware/esx.conf</p><p>/net/vmkernelnic/child[0002]/portgroup = &ldquo;vSAN&rdquo;</p><p>/net/vswitch/child[0002]/portgroup/child[0000]/name = &ldquo;vSAN&rdquo;</p><p>/firewall/services/vsanvp/enabled = &ldquo;true&rdquo;</p><p>/firewall/services/vsanvp/allowedall = &ldquo;true&rdquo;</p><p>/vsan/faultDomainVersion = &ldquo;2&rdquo;</p><p>/vsan/hostDecommissionVersion = &ldquo;0&rdquo;</p><p>/vsan/faultDomainName = ""</p><p>/vsan/subClusterUuid = &ldquo;523a032b-4d44-8fbd-2954-74154216d6b2&rdquo;</p><p>/vsan/autoClaimStorage = &ldquo;true&rdquo;</p><p>/vsan/enabled = &ldquo;true&rdquo;</p><p>/vsan/hostDecommissionMode = &ldquo;decom-mode-none&rdquo;</p><p>/vsan/hostDecommissionState = &ldquo;decom-state-none&rdquo;</p><p>/vsan/datastoreName = &ldquo;vsanDatastore&rdquo;</p><p>/vsan/network/child[0000]/ttl = &ldquo;5&rdquo;</p><p>/vsan/network/child[0000]/vmknic = &ldquo;vmk2&rdquo;</p><p>/vsan/network/child[0000]/masterPort = &ldquo;12345&rdquo;</p><p>/vsan/network/child[0000]/masterGroup = &ldquo;224.1.2.3&rdquo;</p><p>/vsan/network/child[0000]/ifaceUuid = &ldquo;26e84855-f8e9-f3ad-5ff8-005056873a81&rdquo;</p><p>/vsan/network/child[0000]/agentGroup = &ldquo;224.2.3.4&rdquo;</p><p>/vsan/network/child[0000]/agentPort = &ldquo;23451&rdquo;</p><p>/adv/Misc/HostName = &ldquo;esxi02-vsan6&rdquo;</p><p>#<strong>esxcfg-advcfg -l | grep -i vsan</strong></p><p>/VSAN/ClomRepairDelay [Integer] : Minutes to wait for absent components to come back before starting repair (REQUIRES clomd RESTART!)</p><p>/VSAN/ClomMaxComponentSizeGB [Integer] : Maximum component size used for new placements (REQUIRES clomd RESTART!))</p><p>/VSAN/DomLongOpTraceMS [Integer] : Trace ops that take more than the specified value in milliseconds</p><p>/VSAN/DomLongOpUrgentTraceMS [Integer] : Urgent trace ops that take more than the specified value in milliseconds</p><p>/VSAN/DomBriefIoTraces [Integer] : Enables a brief set of per-IO DOM traces for debugging</p><p>/VSAN/DomFullIoTraces [Integer] : Enables full set of per-IO DOM traces; if disabled, IO op traces go to the per-op trace table</p><p>/VSAN/TraceEnableDom [Integer] : DOM tracing enabled</p><p>/VSAN/TraceEnableDomIo [Integer] : DOMIO tracing enabled</p><p>/VSAN/TraceEnableLsom [Integer] : LSOM tracing enabled</p><p>/VSAN/TraceEnableCmmds [Integer] : CMMDS/CMMDSResolver tracing enabled</p><p>/VSAN/TraceEnableRdt [Integer] : RDT tracing enabled</p><p>/VSAN/TraceEnablePlog [Integer] : PLOG tracing enabled</p><p>/VSAN/TraceEnableSsdLog [Integer] : SSDLOG tracing enabled</p><p>/VSAN/TraceEnableVirsto [Integer] : Virsto tracing enabled</p><p>/VSAN/VsanSparseEnabled [Integer] : Enable auto-creation of vsanSparse instead of vmfsSparse redologs, for VSAN 2.0 datastore only</p><p>/VSAN/VsanSparseCacheThreshold [Integer] : Maximum number of entries in single VsanSparse cache</p><p>/VSAN/VsanSparseCacheOverEvict [Integer] : Percentage of VsanSparseCacheThreshold to add to eviction</p><p>/VSAN/VsanSparseSpeculativePrefetch [Integer] : Number of bytes to add onto each extent interrogation request</p><p>/VSAN/VsanSparseMaxExtentsPrefetch [Integer] : Maximum number of extents to fetch during interrogation</p><p>/VSAN/VsanSparseParallelLookup [Integer] : Request written extent data from each layer in parallel</p><p>/VSAN/TraceEnableVsanSparse [Integer] : VsanSparse tracing enabled</p><p>/VSAN/TraceEnableVsanSparseIO [Integer] : VsanSparse per-IO tracing enabled</p><p>/VSAN/TraceEnableVsanSparseVerbose [Integer] : VsanSparse very verbose tracing enabled</p><p>/VSAN/VsanSparseHeapSize [Integer] : Maximum heap size for VsanSparse snapshot consolidation buffers(in KiB)</p><p> </p><p>Next time I will tell you how to upgrade your vSan Cluster to vSAN 6.0.</p><p><strong>Stay tuned!</strong></p><div class=nav-next-prev><div class=nav-prev><a href=https://vovando.dev/2015/05/06/set-coredump-partition-on-esxi/><i class="fas fa-chevron-left"></i></a></div><a class=nav-top href=#>top</i></a><div class=nav-next><a href=https://vovando.dev/2015/05/07/upgrade-esxi-to-6-0-with-vmware-update-manager/><i class="fas fa-chevron-right"></i></a></div></div></div><footer><div class=footer-content><div class=contact-info><div class=footer-mail><i class="far fa-envelope"></i><a href=mailto:hello@vovando.dev>hello@vovando.dev</a></div><div class=footer-phone><i class="fas fa-phone"></i>7777</div></div><p class="copyright meta">Copyright © 2013–2020, all rights reserved.</p></div></footer></main></body><script src=https://vovando.dev/js/navbutton.js></script></html>